\chapter{Conclusion}
\label{chapter:conclusion}

%\minitoc
\chapterwithfigures{\nameref*{chapter:conclusion}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipConclusion}}{\endinput}{}

We now summarize the contributions of this thesis and offer some future directions of Continual
Learning.

\section{Contributions}

During this thesis, we aim to learn an increasing number of classes with \ac{DL} architectures for
\ac{CV} without forgetting. We design multiple methods to achieve this goal, with a particular
interest on how the visual features of a continual model evolve through time. First, in
\autoref{chapter:regularization}, we investigate how to constrain features while satisfying a
rigidity-plasticity trade-off. Then, in \autoref{chapter:segmentation}, we explore continual
approaches for semantic segmentation. Finally, in \autoref{chapter:dynamic}, we exploit the
transformer architecture in a dynamic framework to condition features per task.

\paragraph{Visual Features regularization} We study in \autoref{chapter:regularization} \ac{CIL} for
image classification. In this setting, regularization constraining a model's output is the most
common approach. We challenge this paradigm by outlining two drawbacks: it balances poorly the
rigidity (not forgetting old classes) \vs plasticity (learning new classes) trade-off. Moreover,
constraining intermediary visual features is a stronger regularization. Then, we design two
feature-based regularizations: (1) PODNet minimizes the drift between statistics of the visual
features between the old and new models. The design of this method explicitly reduces forgetting
while letting enough slack to efficiently learn new classes. (2) Our second approach, Ghost, avoids
forgetting before it even happens by pre-allocating areas of the latent space for future classes by
drawing inspiration from the zeroshot literature. For this second approach, we propose the Prescient
Continual Learning setting where detailed attributes of each class are available. This is a
reasonable assumption in the fashion context of Heuritech, the company sponsoring this PhD.

\paragraph{Continual Semantic Segmentation} We explore \ac{CSS} in \autoref{chapter:segmentation}.
We highlight the two main challenges: an important catastrophic forgetting linked to the higher
complexity of segmentation images, and a background shift where only classes of the current task are
labeled. To reduce the catastrophic forgetting of old classes, inspired by our previous POD, we
present a multi-scale distillation loss that constrains local regions of visual features. Then, to
tackle the background shift, we design an uncertainty-based hard pseudo-labeling loss. We show that
despite its usefulness, our pseudo-labeling can fail for particular situations, and complement it
with an efficient object rehearsal method.

\paragraph{Dynamic Strategy with Transformers} Finally, in \autoref{chapter:dynamic}, we propose to
use the recent transformer architecture with a dynamic strategy in \acf{CIL} for image
classification. Previous dynamic networks, that expand their parameters as the number of learned
tasks increases, struggle to limit their memory and time overheads. We propose instead to share a
common encoding produced by self-attention layers, and to condition the features for each task using
task-specific tokens. This architecture allows us to dynamically process new tasks with very little
memory overhead even when faced to a large number of tasks.

\section{Future Work}

We now discuss some future directions of our work, both with respect to the data/benchmark and to
the architecture/optimization process.

\vspace{2em}
\noindent{\large{\textbf{Data \& Benchmarks}}}

\paragraph{Time Limitation} Current works in the literature, including this thesis, focus on using
no rehearsal data or at least very few. The common justifications are about private data that cannot
be kept, or embedded device with little storage. In many situations, those constraints are
realistic. However, in other situations, given large data centers, storage capacity is less a
problem. In that case, the main constraint is time: continually learn new data should be
significantly faster than retraining from scratch. Thus, a new setting with large-scale rehearsal
memory for continual learning would impose a time budget \citep{veniat2018budgetedlearning}.

\paragraph{Universal Representation} A major cause of forgetting in deep neural networks is that
they learn spurious features \citep{lesort2022spuriousfeatures}, useful for the current task, but
that may cause interference with future tasks. Ideally, a network should learn invariants
\citep{arjovsky2019irm,rame2021fishr} that generalizes better to new distributions. Recently, it has
been proposed that self-supervision could learn more class-agnostic features and in turns
drastically reduce forgetting \citep{gallardo2021selfsupcontinual}. Universal
features, trained in self-supervision, with a metric-based approach, on a wide diversity of
modalities (RGB, depth, \etc), will enable better continual models. First, they would forget less
because the need to adapt the representation to new tasks is reduced. Second, an adaptation to new
tasks will be extremely quick, where simply tuning a task token in a Dytox-like strategy would
result in good performances.

\vspace{2em}
\noindent{\large{\textbf{Architecture \& Optimization}}}

\paragraph{Deep Architectures} for continual learning is a fewly explored topic as most works focus
on a fixed MLP or a ResNet-18. Initial findings remark that larger networks forget less
\citep{ramasesh2022scalecontinual}, especially when scaling the width
\citep{mirzadeh2022widecontinualnetworks}. Going further than simply scaling, one may wonder if the
structure of current architectures, optimized for \iid training, have inherent flaws regarding
continual learning. More particularly, Mixture-of-Experts (MoE) is an interesting direction for
continual learning \citep{caccia2022anytimelearning}: large models could learn universal features
that could be conditioned with task-specific experts. The usage of experts does not necessarily need
to be exclusive to a particular task: in fact a form of compositionality could be obtained where
each expert specializes a some concept. This compositionality could speed up the learning of new
tasks with little forgetting without requiring an important adaptation of the parameters.

\paragraph{Second-order optimization} methods can help deep neural networks to find wide local
minima, which could encompass multiple task local minima \citep{lee2020kroneckercontinual}. As a
result, the parameter drift between the optimal weights of a task $t$ and $t+1$ can be minimal, and
in turn avoid the bulk of the forgetting. We briefly explore this direction with the
Sharpness-Aware-Minimizer \citep{foret2020sam} in this thesis, but more work could be done on this
topic. An important drawback of current second-order methods is obviously the higher computational
cost they incur, thus faster alternative methods also seeking flat wide minima such as
\citet{cha2021swad}'s SWAD could be explored.

\paragraph{Learning differently} The vast majority of deep architectures, in all domains, are
trained with gradient descent (including fancier optimizers as Adam or LAMB). The major drawback of
this update rule is the ``\textit{tug-of-war}'' \citep{hadsell2020embracingchange} where the
gradient from each task pulls the solution towards its optimum. Different update rules, more closely
inspired from the biological brains, can be an axis of investigation, as Hebbian learning for
continual learning \citep{taylor2020hebbiancontinual}. Moreover, instead of designing explicitly an
update rule, it could be meta-learned. This meta-learning could aim faster remembering of previous
forgotten tasks \citep{he2019metacontinual,caccia2020osaka}.


