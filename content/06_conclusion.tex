\chapter{Conclusion}
\label{chapter:conclusion}

%\minitoc
\chapterwithfigures{\nameref*{chapter:conclusion}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipConclusion}}{\endinput}{}

We now summarize the contributions of this thesis and offer some future directions of Continual
Learning.

\section{Contributions}

During this thesis, we aim to learn an increasing number of classes with \ac{DL} architectures for
\ac{CV} without forgetting. We design multiple methods to achieve this goal, with a particular
interest on how the visual features of a continual model evolve through time. First, in
\autoref{chapter:regularization}, we investigate how to constrain features while satisfying a
rigidity-plasticity trade-off. Then, in \autoref{chapter:segmentation}, we explore continual
approaches for semantic segmentation. Finally, in \autoref{chapter:dynamic}, we exploit the
transformer architecture in a dynamic framework to condition features per task.

\paragraph{Visual Features regularization} We study in \autoref{chapter:regularization} \ac{CIL} for
image classification. In this setting, regularization constraining a model's output is the most
common approach. We challenge this paradigm by outlining two drawbacks: it balances poorly the
rigidity (not forgetting old classes) \vs plasticity (learning new classes) trade-off. Moreover,
constraining intermediary visual features is a stronger regularization. Then, we design two
feature-based regularizations: (1) PODNet minimizes the drift between statistics of the visual
features between the old and new models. The design of this method explicitly reduce forgetting
while letting enough slack to learn efficiently new classes. (2) Our second approach, Ghost, avoids
forgetting before it even happens by pre-allocating areas of the latent space for future classes by
drawing inspiration from the zeroshot literature. For this second approach, we propose the Prescient
Continual Learning setting where detailed attributes of each class are available. This is a
reasonable assumption in the fashion context of Heuritech, the company sponsoring this Ph.D.

\paragraph{Continual Semantic Segmentation} We explore \ac{CSS} in \autoref{chapter:segmentation}.
We highlight the two main challenges: an important catastrophic forgetting linked to the higher
complexity of segmentation images, and a background shift where only classes of the current task are
labeled. To reduce the catastrophic forgetting of old classes, inspired by our previous POD, we
present a multi-scale distillation loss that constrains local regions of visual features. Then, to
tackle the background shift, we design an uncertainty-based hard pseudo-labeling loss. We show that
despite its usefulness, our pseudo-labeling can fails for particular situations, and complement it
with an efficient object rehearsal method.

\paragraph{Dynamic Strategy with Transformers} Finally, in \autoref{chapter:dynamic}, we propose to
use the recent transformer architecture with a dynamic strategy in \acf{CIL} for image
classification. Previous dynamic networks, that expand their parameters as the number of learned
tasks increases, struggle to limit their memory and time overheads. We propose instead to share a
common encoding produced by self-attention layers, and to condition the features for each task using
task-specific tokens. This architecture allows us to dynamically process new tasks with very little
memory overhead even when faced to a large number of tasks.

\section{Future Work}

When discussing the future directions of the Continual Learning field, we will distinguish between
those related to the data and those to the architectures \& losses themselves.

We now discuss future directions of the Continual Learning field and how our contributions could be
extended.

\paragraph{Time Limitation:} Current works in the literature, including this thesis, focus on using
no rehearsal data or at least very few. The common justifications are about private data that cannot
be kept, or embedded device with little storage. However, an important part of the industry is not
concerned by neither constraints: Private data can be siloed to avoid leakage to the public and
storage is now a cheap commodity. The real constraint is time: continually learn new data should be
significantly faster than retraining from scratch. Thus, a new more realistic setting for Continual
Learning would impose a time budget \citep{veniat2018budgetedlearning}.

\paragraph{Universal Representation:} A major cause of forgetting in deep neural networks is that
they learn spurious features \citep{lesort2022spuriousfeatures}, useful for the current task, but
that may cause interference with future tasks. Ideally, a network should learn invariants
\citep{rame2021fishr} that generalizes better to new distributions. Recently, it has been proposed
that self-supervision could learn more class-agnostic features and in turns reduce drastically
forgetting \citep{gallardo2021selfsupcontinual}. We believe that universal features, trained in
self-supervision, with a metric-based approach, on a wide diversity of modalities (RGB, depth,
\etc), will enable better continual models. First, they would forget less because the need to adapt the
representation to new tasks is reduced. Second, an adaptation to new tasks will be extremely quick,
where simply tuning a task token in a Dytox-like strategy would result in good performances.

\paragraph{Mixture-of-Experts (MoE)} is a combination of multiple small models into a larger model. For
transformers replace the MLP in the Self-Attention Block by multiple small MLPs. A routing system
redirects each tokens/dimensions to a particular \textit{expert} MLP. Applying MoE to continual
learning has been yet fewly explored \citep{caccia2022anytimelearning}. We hypothetize that large
models could learn universal features (see previous point) that could be conditioned with
task-specific experts. The usage of experts does not necessarily need to be exclusive to a
particular task: in fact a form of compositionality could be obtained where each expert specializes
a some concept. We believe that this compositionality could speed up the learning of new tasks with
few adaptations, and thus one may hope, with little forgetting.

\paragraph{Task-specific Attention Heads}


\paragraph{Large-scale training \& models} \citep{ramasesh2022scalecontinual}

\paragraph{Multi-modalities}

\paragraph{Second-order optimization}
