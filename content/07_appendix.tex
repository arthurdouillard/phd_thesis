\chapter{Appendix: more papers}
\label{chapter:appendix}

%\minitoc
\chapterwithfigures{\nameref*{chapter:appendix}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipAppendix}}{\endinput}{}

\section{Specific variations of Continual Learning}
\label{sec:related_variation}

While \acf{CIL}, detailed in \autoref{chapter:related}, is the most common benchmark in Continual
Learning, there are multiple variations of benchmark regardless which kind of shift is involved.

\subsection{Multiple labels}
\label{sec:related_multiple_labels}

The main task in Continual Learning is classification of a single class per sample, however it can
also be expanded to multiple classes per samples, \eg object detection
\citep{shmelkov2017incrementalobjectdetection} and semantic segmentation
\citep{michieli2019ilt,cermelli2020modelingthebackground}. The latter has seen recently interest
from the community for its concrete application: hand labeling in segmentation is extremely costly,
and continual segmentation propose to labelize only the new classes in an image, reducing greatly
the labeling cost. In that situation, a segmentation maps (made of one label per pixel) will only be
partially labelized: new classes are labelized, but old classes are assumed to be background.
Moreover, our model may have encountered new classes in the past, when they were themselves
considered as background. It's a case of concept shift, where the conditional distribution $p(y |
      x)$ changes through time. I detail in \autoref{chapter:segmentation} the existing benchmark in
\ac{CSS} and describe how we tackled this problem.

\subsection{Online Learning \& Task drift detection}
\label{sec:related_online_learning}

In Continual Learning, a model learns for multiple epochs for each task. On the other hand, in
Online Learning, also called Stream Learning, there are no notions of tasks nor epochs: a model must
learn on a stream of samples incoming one by one, and which cannot be replayed by epochs \citep{aljundi2019notaskboundaries}. The
methods to reduce forgetting described in \autoref{sec:related_methods} can still be applied in
Online Learning. Modified versions of rehearsal learning, often inspired by reservoir sampling \cite{knuth97tacpvol2}, are
often used \cite{hayes2019exstream,aljundi2019taskfree}.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.6\linewidth]{images/related/loss_drift.png}
      \end{center}
      \caption{\textbf{Task-free detection of drift in the input distribution} by recording the
            plateau in the loss followed by a peak. y-axis is the loss value, and x-axis the update steps.
            Image from \cite{aljundi2019taskfree}.}
      \label{fig:related_lossdrift}
\end{figure}

Multiple regularization methods (\autoref{sec:related_regul}) needs to do some computation between
tasks. For example, weight-based regularization (\autoref{sec:related_regul_weight}) must compute
the task-specific importance weights. Because in Online Learnig, there are no clear task separation,
a heuristic must determine when doing this computation. \cite{aljundi2019taskfree}, working a stream
of images from soap operas, proposed to analyze the loss surface to find drift in the distribution:
at some point the model is experienced enough, and the loss starts to plateau. When a drift happens,
the loss will usually peak. This is a sign of task-free drift of the distribution as
illustrated in \autoref{fig:related_lossdrift}.

\subsection{Continual-Meta Learning}
\label{sec:related_meta}


Continual Learning aims to not forget. However, we --as humans-- often forget, but we can also
re-learn what was lost quicker than the first time. The goal of \ac{MCL} is therefore to recover as
quickly as possible --sample wise-- the original performance on past tasks
\citep{he2019metacontinual}. As the name implies, meta-learning methods, that aims to \textit{learn
      how to learn}, such as the MAML model \citep{finn2017maml} are used to that end. Then, \ac{MCL} has
been extended to a more general framework where the model also has to adapt quickly to new \acf{OoD}
tasks.

Note that \acf{MCL} is not to be confused with \acf{CML} where in that case meta-learning is only
used during pretraining to provide better model initialization.

\subsection{Zeroshot Continual Learning}
\label{sec:related_zeroshot}


In Computer Vision, \acf{ZSL} \citep{lampert2009zeroshot,xian2019awa2} aims to classify classes that
were never seen before. To do so, models usually exploit an external knowledge source as a word2vec
embedding \citep{mikolov2013word2vec} trained on Wikipedia or an attribute matrix. Several works
have proposed to unify both Continual Learning and \ac{ZSL} where the future classes that haven't
been seen yet must be classified
\cite{lopezpaz2017gem,wei2020lifelongzeroshot,gautam2020continualzeroshot}. Rather than simply using
\ac{ZSL} as end to itself, we proposed, in \citep{douillard2020ghost}, to exploit its properties to
directly avoid forgetting. This is elaborated further in \autoref{chapter:regularization}
(\autoref{sec:ghost}).

\subsection{Natural Language Processing}
\label{sec:related_nlp}


Continual Learning can be applied to all modalities. After \acf{CV}, the most common one is
\ac{NLP}. \ac{NLP} saw its "\textit{ImageNet moment}" with the advent of Transformers
\citep{vaswani2017transformer}, and more recently with multi-tasks learning \citep{raffel2019t5}.
Continual \ac{NLP} \cite{biesialska2020continualnlp} aims naturally to learn multiple tasks, but in
a consecutive fashion with no --or few-- replay of the old tasks data. Applications can be similar
to \ac{CV} with addition of new classes \citep{masson2019episodiclifelongnlp} or new domains (\eg
medical corpus, fiction, tweets, etc.) \citep{gerald2021continualri}.

\subsection{Reinforcement Learning}
\label{sec:related_rl}


\ac{RL} \citep{sutton1998rl} more often than not needs support from Continual
Learning \citep{khetarpal2020continualrl}: for example as an agent evolves in an environment, it
usually needs rehearsal learning (also known as episodic memory) \citep{mnih2013atarirl}. Overall,
the methods originally developped for \ac{CV} (\autoref{sec:related_methods}) were then applied to
both \ac{NLP} and \ac{RL}.


\section{Details on PODNet}
\label{sec:appendix_podnet}

\subsection{Implementation details}

For all datasets, images are augmented with random crops and flips. For CIFAR100, we additionally
change image intensity by a random value in the range [-63, 63].
%
We train our model for 160 epochs for CIFAR100, and 90 epochs for both ImageNet100 and ImageNet100,
with a SGD optimizer with momentum of 0.9. For all datasets, we start with a learning rate of 0.1, a
batch size of 128, and cosine annealing scheduling.
%
The weight decay is $5\cdot 10^{-4}$ for CIFAR100, and $1\cdot 10^{-4}$ for ImageNet100 and
ImageNet1000. For CIFAR100 we set model hyperparameters $\lambda_c = 3$ and $\lambda_f=1$, while for
ImageNet100 and 1000 we set $\lambda_c = 8$ and $\lambda_f =10$. Our model uses POD-spatial and
POD-flat except when explicitly stated otherwise. Following Hou et al.~\cite{hou2019ucir}, we
multiply both losses by the adaptive scaling factor: $\lambda=\sqrt{\nicefrac{N}{T}}$ with $N$ being
the number of seen classes and $T$ the number of classes in the current task.

For POD-spatial, before sum-pooling we take the features to the power of 2 element-wise. The vector
resulting from the pooling is then L2 normalized.

\subsection{Number of proxies per class}

While our model's expressiveness increases with more proxies in $\mcL_\text{LSC}$, it remains fairly
stable for values between 5 and 15, thus, for simplicity, we kept it fixed to 10 in all experiments.

In initial experiments, we had the following pairs for the number of clusters (k) and average
incremental accuracy (acc): k=1, acc=56.80\%; k=2, 57.14\%; k=4, acc=57.40\%; k=6, acc=57.46\%; k=8,
acc=57.95\%, and k=10, acc=57.98\% --- i.e., a 1.18 p.p. improvement moving from k=1 to k=10. On
ImageNet100, with 10 steps/tasks (increments of give classes per task), moving from k=1 to k=10
improved 1.51 p.p. on acc.

\subsection{Reproducibility}

\paragraph{Code Dependencies} The Python version is  3.7.6. We used the PyTorch
\cite{paszke2017pytorch} (version 1.2.0) deep learning framework and the libraries Torchvision
(version 0.4.0), NumPy \cite{oliphant2006numpy} (version 1.17.2), Pillow (version 6.2.1), and
Matplotlib \cite{hunter2007matplotlib} (version 3.1.0). The CUDA version is 10.2. Initial
experiments were done with the data loaders library Continuum \cite{douillardlesort2020continuum}.
PODNet's full code is released at:\\
\href{https://github.com/arthurdouillard/incremental\_learning.pytorch}{\texttt{github.com/arthurdouillard/incremental\_learning.pytorch}}.
\\We provide all configuration files necessary to reproduce results, including seeds and class
ordering.

\paragraph{Datasets description} I provide bellow extensive details on the content of the three
datasets considered for PODNet: CIFAR100, ImageNet100, and ImageNet1000.

{\begin{description} \setlength{\parskip}{0pt}
    \item[CIFAR100] contains 32$\times$32-pixel images in 100 classes, with 50k images for training
          and 10k for testing.
    \item[ImageNet100] contains 224$\times$224-pixel images in 100 classes, with $\sim$128k images
          for training and $\sim$5k for testing.
    \item[ImageNet1000] contains 224$\times$224-pixel images in 1000 classes, with $\sim$1.28m
          images for training and $\sim$50k for testing. \end{description}}

\paragraph{Spatial-based distillation} I displayed the differences of performance between
spatial-based distillation in \autoref{sec:podnet_ablation_pooling}
(\autoref{tab:podnet_ablation_perceptual}) when combined with POD-flat. In this appendix, I also
detail in \autoref{tab:podnet_ablation_perceptual_noflat} the same spatial-loss without POD-flat.
The ranking between distillation losses is ostensibly the same. Notice that POD-spatial ---and its
sub-components POD-width and POD-height-- are the only losses barely affected by POD-flat's absence.
Note that all alternative losses were tuned on the validation set to get the best performance,
including those from external papers. Still, our proposed loss, POD-spatial, outperforms all, both
with and without POD-flat.


\input{tables/podnet/ablation_pod_woflat.tex}


\section{Details on Ghost}
\label{sec:appendix_ghost}


\subsection{Overhead of SVMs training}

Training the SVMs for $\mcL^{\text{\tiny{svm-reg}}}$ introduces a computational overhead. To
minimize it, we limit the number of features per class to 500. Moreover, as we advance towards later
tasks, fewer unseen classes remain, and thus we have fewer SVMs to train. Overall, an experiment on
AwA2, with our setting of 25 classes + 5 $\times$ 5 classes, takes 5 hours to train. We observed
that our SVM-based regularization extends that time by less than 5 minutes on average, an overhead
of less than 2\%, which we deemed acceptable. For reference, the SVMs were trained on a machine with
10 CPU cores of 3.90GHz each.

\subsection{Implementation Details} For all datasets and settings, we set the classification margin
$\delta=0.6$, and the SVM latent-space regularization additional margin $\tau=1$. We train the
feature-extractor-and-classifier pipeline for 90 epochs with an SGD optimizer, learning rate of 0.1,
cosine scheduling, and weight decay of $10^{-4}$. We train the generator for 1200 epochs, with an
Adam optimizer and a learning rate of $10^{-5}$. Finally, following
\cite{hou2019ucir,douillard2020podnet}, we fine-tune the classifier for 60 epochs (with the feature
extractor frozen and a small learning rate of $10^-4$) at the end of every task (except the last
one). We found useful to balance the bias towards the seen classes against the unseen classes. With
the POD distillation \cite{douillard2020podnet}, we set $\lambda_1=3$  for AwA2, and $\lambda_1=15$
for aP\&Y; with the Less-Forget distillation \cite{hou2019ucir}, we set $\lambda_1=4$ for both
datasets. We always set $\lambda_2=10^{-3}$, moreover we apply it on L2-normalized features.
Finally, we do not reinitialize the models between tasks: $f^t$ results from training $f^{t-1}$ on
task $t$, etc. On the rehearsal memory limitation, we follow the strict setting of Hou et
al.~\cite{hou2019ucir}, keeping only $s=20$ training images per past class.

\subsection{Datasets details}

We train our model on three datasets: MNIST, AwA2, and aP\&Y. Baselines and our Ghost models are run
on the exact same data/class splits, with the exact same preprocessing.

\paragraph{MNIST} This dataset has ten classes: handwritten digits ranging from '0'' to '9'. It has
a training set of 60,000 images and a test set of 10,000 images. We used for validation set, a
subset of 10,000 examples of the training set. Images are in black\&white (one channel) and of
dimension $28\times28$. We convert the pixels values to the range [0, 1] and then normalize by the
mean and standard deviation of the training dataset.

\paragraph{AwA2} This dataset has 50 animals classes. It has a training set of 29,857 images and a
test set of 7,465 images. We used for validation set a subset of 8,000 images of the training set.
Images are in RGB color. We convert the pixel values to the range [0, 1] and normalize by the mean
and standard deviation of the training dataset. Train images are randomly cropped to a square of
$224\times224$ and are randomly flipped horizontally. Test images are resized to $256\times256$ and
then center cropped to $224\times224$.

\paragraph{aP\&Y} This dataset has 32 classes of everyday objects. It has a training set of 12,269
images and a test set of 3,068 images. We used for validation set a subset of 4,000 images of the
training set. Images are in RGB color. We convert the pixel values to the range [0, 1] and normalize
by the mean and standard deviation of the training dataset. Train images are randomly cropped to a
square of $224\times224$ and are randomly flipped horizontally. Test images are resized to
$256\times256$ and then center cropped to $224\times224$.

\subsection{Reproducibility}

\paragraph{Code Dependencies} The Python version is  3.7.6. We used the PyTorch
\cite{paszke2017pytorch} (version 1.2.0) deep learning framework and the libraries Torchvision
(version 0.4.0), NumPy \cite{oliphant2006numpy} (version 1.17.2), Pillow (version 6.2.1), and
Matplotlib \cite{hunter2007matplotlib} (version 3.1.0). The CUDA version is 10.2. Experiments on
MNIST were done with the data loaders library Continuum \cite{douillardlesort2021continuum}.

The code is released at
\href{https://github.com/arthurdouillard/incremental_learning.pytorch}{github.com/arthurdouillard/incremental\_learning.pytorch}.

\paragraph{Hardware \& Training duration} We ran our experiments on 3 Titan Xp GPUs with 12 Go of
VRAM each. Each experiment had access to 10 CPU cores of 3.90 GHz each, and used at most 3 Go of RAM
and 8 Go of VRAM. A single experiment run on AwA2 took on average 5 hours and, on aP\&Y, 3 hours. We
ran each experiment thrice with different random seeds (1, 2, and 3).

\section{Continuum: Continual Learning Data Loaders}

\section{CTKT: Continual Domain Adaptation}

\section{Scaling Laws for Continual Learning}


