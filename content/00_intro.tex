\chapter{Introduction}
\label{chapter:introduction}

%\minitoc
\chapterwithfigures{\nameref*{chapter:introduction}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipIntro}}{\endinput}{}


% what is ai
% application of ai
% ethical limits

% computer vision, input/output
% M/D Learning, abstract, no math
% heuritech
% continual learning what are the problems
% outline, maybe fusionned with CL

\epigraph{I believe that at the end of the century, the use of words and general educated opinion
      will have altered so much that one will be able to speak of machines thinking without expecting to
      be contradicted.}{\textit{Alan Turing}}

In this thesis introduction, using layman terms, I describe Artificial Intelligence, and,
in particular, one of its instances: Deep Learning. Then, I lay out the challenges of this
thesis and my contributions.

%\section{Artificial Intelligence}

\textbf{The idea of thinking machines} began in the previous century, from Karel Çapek's invention
of the "\textit{robot}" to the 1956's Dartmouth workshop passing by Turing \& von Neumann's
reflections. Despite suffering from multiple "AI winters" filled with disappointments and
criticisms, Turing's prediction on the rising importance of \ac{AI} proved to be right as the first
and the second decades of the XXI century saw the advent of respectively \acf{ML}
\citep{bishop2006prml} and \acf{DL} \citep{goodfellow2016deeplearningbook}, two major subfields of
\ac{AI}, related to statistical learning theories.

Providing \textbf{a definition of \ac{AI}} is difficult, but its foremost domains, \ac{ML} and
\ac{DL}, can be defined as statistical algorithms that can improve automatically through experience
and the use of data. These methods are already ubiquitous: speech recognition enabling us to control
devices remotely \citep{amodei2016deepspeech2}, recommender systems proposing movies according to
our taste \citep{toscher2009netflixprize}, automatic translation \citep{vaswani2017transformer},
face recognition \citep{schroff2015facenet}, autonomous driving \citep{sun2020waymodataset}, \etc.
Less known but still useful applications comprise accelerated physics simulation
\citep{breen2020threebody}, protein folding prediction \citep{jumper2021alphafold}, molecule
toxicity estimation \citep{nih2019toxchallenge}, data center cooling
system \citep{evans2016datacentercooling}, control of the magnetic coils of a nuclear fusion reactor
\citep{degrave2022nuclearreactor}, \etc.

\ac{AI} is increasingly more important in our daily lives with some applications raising \textbf{ethical
      concerns}: face recognition biased towards some populations, loan grants, medical diagnosis, justice
advice, biased chatbots \citep{sheng2019lmbias}, \etc. Therefore, we must pay a particular interest in the
potential impact of this new technology. Towards this goal, people from diverse backgrounds
must participate in the creation of such technology, and standardization bodies should
advise what types of systems can be used in which scenarios \citep{gebru2019aiethichandbook}.


\section{PhD Thesis Context}

Now with the type of methods and data (Deep Learning and images) defined, I contextualize my
thesis with relation to my sponsor, how it influenced our research, and what challenges we have aimed to tackle.

\paragraph{Heuritech} This thesis was sponsored by the Parisian startup
Heuritech\footnote{\url{https://heuritech.com}} as a \textit{CIFRE PhD}. The company analyzes social
networks such as Instagram and Weibo, recognizes the clothes in pictures, estimates
volumes of fine-grained types of garments, and finally forecasts future trends. The company's
\acf{CV} models must recognize an ever-growing number of entities from features (\eg knitted, blue
color, short cut) to brand models (\eg Nike Air Max, Adidas Stan Smith, Puma Suede). This
requirement leads to two problems: \textbf{(1)} the time spent to re-train a model is growing
linearly, and \textbf{(2)} learning a new entity can incur a performance loss on previously learned
entities.% The scientific literature names the domain of learning new concepts incrementally
%``\textbf{Continual Learning}'', and the loss of performance ``\textbf{Catastrophic Forgetting}''.

\paragraph{Continual Learning} is a field that emerged in the 1990s but saw renewed interest only
very recently around the second half of the 2010s. The goal is to deal with datasets that evolve
through time. This evolution can take many forms, including adding new entities to predict in a
classification task (\eg learning sneaker brands, then high-heel brands) or adding samples from new
sources (\eg commercial photoshoot then images from social media). Unfortunately, current
State-of-the-Art models struggle to learn continually new data without losing performance on
previously seen data. This loss is so critical that the literature nicknamed it a ``catastrophic
forgetting'' \citep{robins1995catastrophicforgetting}. Multiple methods can reduce
this forgetting, including rehearsal and constraints. Rehearsal involves reviewing
previously learned knowledge, as a human student would \textit{rehearse} the last semester's course.
However, this rehearsal is often limited in order to reduce computational cost and because past data
may not always be available for a variety of reasons, including privacy. On the other hand,
constraints enforce the model to keep a similar \textit{behavior} as it learns new concepts, but
defining the optimal constraint is not trivial.

\section{Contributions}

This PhD thesis is focused on bringing Continual Learning capacity to
Computer Vision models. In the next chapter (\autoref{chapter:related}), We detail the learning
procedure, state the exact benchmarks considered in Continual Learning, and review the
State-of-the-Art methods in this domain. Then, we cover the work we have done during this thesis over
three chapters as follows:

\begin{itemize}
      \item \autoref{chapter:regularization}: \nameref{chapter:regularization}\\
            I first review the existing methods based on regularization for continual learning. While
            regularizing a model's probabilities is very efficient to reduce forgetting in large-scale
            datasets, there are few works considering constraints on intermediate features. I cover in this
            chapter two contributions aiming to regularize the latent space of \acs{ConvNet} directly. The
            first one, \acf{PODNet} aims to reduce the drift of spatial statistics between the old and new
            model, which in effect reduces drastically forgetting even on an important amount of tasks. I show in a
            second part a complementary method where we avoid pre-emptively forgetting by allocating
            locations in the latent space for yet unseen future class.
      \item \autoref{chapter:segmentation}: \nameref{chapter:segmentation}\\
            Then, I describe a recent application of \acf{CIL} to semantic segmentation. I show that
            the very nature of \acf{CSS} offers new specific challenges, namely forgetting on large
            images and a background shift. We tackle the first problem by extending our distillation
            loss introduced in the previous chapter to multi-scales. The second problem is solved by
            our efficient pseudo-labeling strategy. Finally, we consider the common rehearsal learning
            but applied this time to \ac{CSS}. I show that it cannot be used naively because of memory
            complexity and design a memory-efficient rehearsal that is even more efficient.
      \item \autoref{chapter:dynamic}: \nameref{chapter:dynamic}\\
            Finally, I consider a completely different approach to continual learning: dynamic networks
            where the parameters are extended during training to adapt to new tasks. Previous works on
            this domain are hard to train and often suffer from parameter count explosion. For the
            first time in continual computer vision, we propose to use the Transformer architecture:
            the model's parameters are shared across tasks except for an
            expansion of learned task tokens. With an encoder/decoder strategy where a task token
            specializes the decoder's forward, we show state-of-the-art robustness to forgetting
            while our memory and computational complexities barely grow.
\end{itemize}

\paragraph{Related Publications:} This thesis is based on the materials published in the following papers:

\begin{itemize}
      \item \fullcite{douillard2020podnet}
      \item \fullcite{douillard2020ghost}
      \item \fullcite{douillard2020plop}
      \item \fullcite{douillard2021objectrehearsal}
      \item \fullcite{douillard2021dytox}
\end{itemize}

I have also published other papers that can be found in the \autoref{chapter:appendix}:

\begin{itemize}
      \item \fullcite{douillardlesort2021continuum}
      \item \todo{Add ICML submission on foundation models with MILA}
      \item \todo{Add CTKT with Antoine}
\end{itemize}

