\chapter{Introduction}
\label{chapter:introduction}

%\minitoc
\chapterwithfigures{\nameref*{chapter:introduction}}
%\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipIntro}}{\endinput}{}


% what is ai
% application of ai
% ethical limits

% computer vision, input/output
% M/D Learning, abstract, no math
% heuritech
% continual learning what are the problems
% outline, maybe fusionned with CL

\epigraph{I believe that at the end of the century, the use of words and general educated opinion
      will have altered so much that one will be able to speak of machines thinking without expecting to
      be contradicted.}{\textit{Alan Turing}}

In this thesis introduction, using layman terms, I describe Artificial Intelligence, and,
in particular, one of its instances: Deep Learning. Then, I lay out the challenges of this
thesis and my contributions.

%\section{Artificial Intelligence}

\textbf{The idea of thinking machines} began in the previous century, from Karel Çapek's invention
of the "\textit{robot}" to the 1956's Dartmouth workshop passing by Turing \& von Neumann's
reflections. Despite suffering from multiple "AI winters" filled with disappointments and
criticisms, Turing's prediction on the rising importance of \ac{AI} proved to be right as the first
and the second decades of the XXI century saw the advent of respectively \acf{ML}
\citep{bishop2006prml} and \acf{DL} \citep{goodfellow2016deeplearningbook}, two major subfields of
\ac{AI}, related to statistical learning theories.

Providing \textbf{a definition of \ac{AI}} is difficult, but its foremost domains, \ac{ML} and
\ac{DL}, can be defined as statistical algorithms that can improve automatically through experience
and the use of data. These methods are already ubiquitous: speech recognition enabling us to control
devices remotely \citep{amodei2016deepspeech2}, recommender systems proposing movies according to
our taste \citep{toscher2009netflixprize}, automatic translation \citep{vaswani2017transformer},
face recognition \citep{schroff2015facenet}, autonomous driving \citep{sun2020waymodataset}, \etc.
Less known but still useful applications comprise accelerated physics simulation
\citep{breen2020threebody}, protein folding prediction \citep{jumper2021alphafold}, molecule
toxicity estimation \citep{nih2019toxchallenge}, data center cooling
system \citep{evans2016datacentercooling}, control of the magnetic coils of a nuclear fusion reactor
\citep{degrave2022nuclearreactor}, \etc.

\ac{AI} is increasingly more important in our daily lives with some applications raising
\textbf{ethical concerns}: face recognition biased towards some populations
\citep{grother2019facerecoethic}, loan grants \citep{anglekar2021loangrantml}, medical diagnosis
\citep{lazzazabal2020medicalbias}, justice advice \citep{russel2020justicefairness}, biased chatbots
\citep{sheng2019lmbias}, \etc. Therefore, we must pay a particular interest in the potential impact
of this new technology. Towards this goal, people from diverse backgrounds must participate in the
creation of such technology, and standardization bodies \citep{tommasi2021fairness} should advise
what types of \ac{AI} systems can be used in which scenarios \citep{gebru2019aiethichandbook}.

\section{Ph.D Thesis Context}

Now with the type of methods and data (Deep Learning and images) defined, I contextualize my
thesis with relation to my sponsor, how it influenced our research, and what challenges we have aimed to tackle.

\paragraph{Heuritech} This thesis was sponsored by the Parisian startup
Heuritech\footnote{\url{https://heuritech.com}} as a \textit{CIFRE Ph.D}. The company analyzes social
networks such as Instagram and Weibo, recognizes the clothes in pictures, estimates
volumes of fine-grained types of garments, and finally forecasts future trends. The company's
\acf{CV} models must recognize an ever-growing number of entities from features (\eg knitted, blue
color, short cut) to brand models (\eg Nike Air Max, Adidas Stan Smith, Puma Suede). This
requirement leads to two problems: \textbf{(1)} the time spent to re-train a model is growing
linearly, and \textbf{(2)} learning a new entity can incur a performance loss on previously learned
entities.

\paragraph{Continual Learning} is a field that emerged in the 1990s but saw renewed interest only
very recently around the second half of the 2010s. The goal is to deal with datasets that evolve
through time. This evolution can take many forms, including adding new entities to predict in a
classification task (\eg learning sneaker brands, then high-heel brands) or adding samples from new
sources (\eg commercial photoshoot then images from social media). Unfortunately, current
State-of-the-Art models struggle to learn continually new data without losing performance on
previously seen data. This loss is so critical that the literature nicknamed it a ``catastrophic
forgetting'' \citep{robins1995catastrophicforgetting,french1999catastrophicforgetting}. Multiple
methods can reduce this forgetting, including rehearsal and constraints. Rehearsal involves
reviewing previously learned knowledge, as a human student would \textit{rehearse} the last
semester's course. However, this rehearsal is often limited in order to reduce computational cost
and because past data may not always be available for a variety of reasons, including privacy. On
the other hand, constraints enforce the model to keep a similar \textit{behavior} as it learns new
concepts, but defining the optimal constraint is not trivial.

\section{Contributions}

This Ph.D thesis is structured around solving catastrophic forgetting in continual settings. We
considered various specific situations and approaches declined in several chapters:

\begin{itemize}
      \item \autoref{chapter:related}: \nameref{chapter:related}\\
            We first briefly cover the neural network architectures used for computer vision. Then,
            we define exactly the existing continual learning benchmarks. Finally, we give an
            overview of the major approaches to solve catastrophic forgetting.

      \item \autoref{chapter:regularization}: \nameref{chapter:regularization}\\
            We first tackle continual learning in the context of the image classification: we aim to
            produce a model that can learn continuously new classes without forgetting the
            previously seen classes. We study how to constrain efficiently a model in order to be
            rigid enough (to not forget) while also being plastic (to learn new classes). The work in
            this chapter has led to two conference publications:
            \begin{itemize}
                  \item \fullcite{douillard2020podnet}
                  \item \fullcite{douillard2020ghost}
            \end{itemize}

      \item \autoref{chapter:segmentation}: \nameref{chapter:segmentation}\\
            Then, we extend our focus to semantic segmentation where each pixel possesses a class.
            We show that this context brings new challenges and propose multiple complementary
            approaches to tackle them. The work in this chapter has led to one conference publication and one
            submission to a journal:
            \begin{itemize}
                  \item \fullcite{douillard2020plop}
                  \item \fullcite{douillard2021objectrehearsal}
            \end{itemize}

      \item \autoref{chapter:dynamic}: \nameref{chapter:dynamic}\\
            Finally, we consider a recent family of neural networks called transformers to build a
            dynamic model for continual image classification. Our goal is to expand the number of
            neurons to efficiently learn new classes in image classification. The work in this
            chapter has led to a conference publication:
            \begin{itemize}
                  \item \fullcite{douillard2021dytox}
            \end{itemize}

      \item \autoref{chapter:conclusion}: \nameref{chapter:conclusion}\\
            Finally, in a last chapter, we summarize the propositions of this thesis and offer
            new perspectives for the field of continual learning.

\end{itemize}

