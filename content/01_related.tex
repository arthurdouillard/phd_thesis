\chapter{Related Works}
\label{chapter:related}

%\begin{chapabstract}
%      Deep Learning is now the major method to learn unstructured data such as images. Computer
%      Vision used for a long time handcrafted convolution kernels, but nowadays, they are learned:
%      each value of the kernel is a neuron. This kind of network, called \acf{CNN}, showed
%      impressive performance in a wide variety of tasks. However, they still suffer from the plague
%      of forgetting when learning a moving distribution as new samples, from new sources or new
%      classes appear. Continual Learning aims to solve this challenge using multiple
%      approaches, including rehearsal and behavior constraints.
%\end{chapabstract}

%\minitoc
\chapterwithfigures{\nameref*{chapter:related}}
\chapterwithtables{\nameref*{chapter:related}}

\ifthenelse{\boolean{skipRelated}}{\endinput}{}


In this chapter, we detail the related work needed to read this thesis. We first briefly
explain the learning procedure in \acf{DL} and how the data are structured. Then, we describe how
\ac{DL} applications and architectures in \acf{CV}. Finally, we introduce the main topic of this thesis
---Continual Learning--- and showcase the challenges, benchmarks, and methods of this domain. The
notations introduced in this chapter and thorough the thesis are listed in the
\hyperref[chap:notations]{Notations chapter}.

\section{Neural Network Learning}

\ac{DL} models are a succession of linear transformations and non-linear functions and are supposed
to be able to approximate any function \citep{gelenbe1999universalapprox} given the correct
configuration. For example, the most simple \ac{DL} model, a \ac{MLP} with a single hidden layer for
classification is:
%
\begin{equation}
      \hat{\vy} = f_\theta(\vx) = \operatorname{softmax}(\vW_o \sigma(\vW_h \vx + \vb_h) + \vb_o))\,,
      \label{eq:intro_mlp}
\end{equation}
%
\noindent with $\vW_h \in \mathbb{R}^{H \times D}$, $\vb_h \in \mathbb{R}^{H}$, $\vW_o \in
      \mathbb{R}^{C \times H}$, $\vb_o \in \mathbb{R}^{C}$ being the parameters and of the network. $\vx
      \in \mathbb{R}^D$ the input data as a vector, and $\tilde{\vy} \in \mathbb{R}^C$ the predicted
probabilities per classes. $\sigma$ is a hidden non-linear activation, often a \ac{ReLU}
($\operatorname{ReLU(x)} = \text{max}(0, x)$), and $\operatorname{softmax}(\tilde{\vy}) =
      \nicefrac{e^{\tilde{\vy}}}{\sum_{i} e^{\tilde{\vy}_i}}$ is the final non-linear activation. This
small neural network is trained to minimize a loss function. In classification, the most common loss is
the cross-entropy:
%
\begin{equation}
      \mcL(\hat{\vy}, \vy) = -\sum_i y_i \log \hat{y}_i\,,
      \label{eq:intro_ce}
\end{equation}
%
\noindent with $\vy$ a one-hot vector of the labels. Finally, to optimize the parameters of the neural
network, we often use the mini-batch \ac{SGD} algorithm or a variation thereof:

\begin{algorithm}
      \begin{algorithmic}[1]
            \Statex \textbf{input:} a dataset $\mathbb{D}$ with pairs of $(\vx, \vy)$
            \Statex \textbf{input:} a loss function $\mcL(\hat{\vy}, \vy)$
            \Statex \textbf{input:} a model function $f_\theta$
            \Statex \textbf{input:} a learning rate $\eta$ and a batch size $b$
            \Statex

            \While{stopping criterion not satisfied}
            \State $\vx$, $\vy$ $\gets$ sample mini-batch of size $b$ from $\mathbb{D}$
            \State Forward pass: $\hat{\vy}$ $\gets$ $f_\theta(\vx)$
            \State Compute loss: $\mcL$ $\gets$ $\mcL(\hat{\vy}, \vy)$
            \State Compute the gradients: $\delta$ $\gets$ $\nabla_\theta \mcL$
            \State Update all parameters: $\theta$ $\gets$ $\theta - \eta \delta$
            \EndWhile
      \end{algorithmic}
      \caption{Procedure to optimize a neural network with gradient descent.}
      \label{algo:intro_sgd}
\end{algorithm}

Almost all \ac{DL} models, in \ac{CV}, in \ac{NLP}, or even in Audio use a form of gradient descent to
optimize differentiable functions.

\section{Deep Architectures for Computer Vision}

Handcrafted convolutions can extract crude patterns such as edges \citep{lowe1999sift}. Unfortunately,
it is complex to design more elaborated convolution kernels. Therefore, researchers have proposed to
learn the convolution kernels as parameters of the neural networks
\citep{fukushima1980neocognitron,lecun1999lenet}. Those networks are then called \acf{CNN}. In 2012,
thanks to a large dataset and more efficient code working on \acp{GPU}, \cite{krizhevsky2012alexnet}
won the ILSVC competition \citep{russakovsky2015imagenet_ilsvrc} where they had to classify a large
dataset ---ImageNet--- made of 1M2 training images and 1000 classes. From that point forward,
multiple improvements were made to \ac{CNN} \citep{ioffe2015batchnorm,he2016resnet} and these
methods have been applied not only to classification but also object detection
\citep{ren20fasterrcnn}, semantic segmentation \citep{chen2018deeplab}, visual question answering
\citep{benyounes2017mutan}, \etc. we describe now in more details the structure of modern
\acp{CNN}, how to train them efficiently. Then, we highlight a contender to \acp{CNN}: the
transformer architecture.

Most \acp{CNN} follow a similar structure with blocks of convolutions and pooling. Usually, the
feature extractor is ended by a \acf{GAP} and followed by a linear classifier
predicting the classes probabilities. \autoref{fig:related_cnn} illustrates this general paradigm.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=\linewidth]{images/related/cnn.pdf}
      \end{center}
      \caption{\textbf{A Convolutional Neural Networks} extracts more complex patterns through its
            succession of convolutions. In \textcolor{orange}{orange} a convolution, in \textcolor{red}{red}
            a pooling, and in \textcolor{green}{green} the classifier. Given an image, the \ac{CNN} can assign to
            each possible class a probability, all summing to 1.
            Detected patterns taken from \cite{olah2017feature}.}
      \label{fig:related_cnn}
\end{figure}

\paragraph{Architectures:}The 2010's decade saw major improvements to \acp{CNN}, both in their
architecture structure and in their training procedure. \cite{srivastava2015highwaynet} and
\cite{he2016resnet} propose residual connections between blocks likewise: $\vy = \vx +
      \sigma(\operatorname{Conv}(\vx))$. It allows training deeper networks by enabling the gradient to
flow more easily up the earliest layers. This type of connection is now quasi-ubiquitous in all
\ac{DL} based architectures. Other architecture changes include using convolutions of different
kernel sizes in parallel as in Inception \citep{szegedy2015inception}, enabling a multi-scales view
of the features. These architectures are depicted in \autoref{fig:related_resnet_inception}.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=\linewidth]{images/related/resnet_inception.pdf}
      \end{center}
      \caption{\textbf{Different CNN architectures:} \textbf{(a)} illustrates a ResNet-like
            architecture where there are residual connections between blocks. Used by the vast
            majority of modern architectures, these connections help the gradient flows unchanged
            and allow training deeper networks. \textbf{(b)} showcases an Inception-like
            architecture where at the same level convolutions with different kernel sizes are used.
            Each detects patterns of different scales.}
      \label{fig:related_resnet_inception}
\end{figure}

\paragraph{Regularizations:} Improved training procedures \citep{wightman2019resnetstrikesback}
enable the high performance of modern \acp{CNN}: optimizers with adaptive learning rate such as Adam
\citep{kingma2014adam}, improved learning rate scheduling, strong data augmentations
\citep{muller2021trivialaugment,hingyi2018mixup,zhong2017erasing}, and regularizations such as
Dropout \citep{gal2016dropout} and stochastic depth \citep{gao2016stochasticdepth}.

\paragraph{Transformers:} While convolution-based neural networks dominated \acf{CV} in the 2010's
decade, in the last few years, the transformer architecture gained interest: it was originally
designed for machine translation in \ac{NLP} \citep{vaswani2017transformer} with an encoder/decoder
structure and a \textit{self-attention} block between the words embeddings of a sentence. Each word is
embedded into a high-dimensional vector named a ``\textit{token}''. The self-attention operation of a
transformer has a quadratic complexity \wrt the number of tokens: in \ac{NLP}, it is manageable
given a small sentence. However, when applying a transformer on images and considering each pixel
as a token, the complexity is too important. \cite{dosovitskiy2020vit}, based on the encoder
structure of BERT \citep{devlin2018bert}, propose instead to consider a patch of pixels as a token,
reducing greatly the number of tokens.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=\linewidth]{images/related/vit.png}
      \end{center}
      \caption{\textbf{The Vision Transformer (ViT):} the image is cropped without overlap and
            projected using a convolution whose stride equals the kernel size. The encoder is made of
            multiple transformer blocks. Finally, only the special learned token ``class token'' is used
            at the end, and fed to a linear classifier. Image from \cite{dosovitskiy2020vit}.}
      \label{fig:related_vit}
\end{figure}

This architecture is illustrated in \autoref{fig:related_vit}. A special learned token, called
``\textit{class token}'', is added to the patch tokens. Then, successive transformer blocks process
all the tokens. Each block is made of LayerNorms \citep{ba2016layernorm}, a self-attention block, a
\ac{MLP}, and residual connections. Thus, the self-attention block is:
%
\begin{equation}
      \begin{aligned}
            Q & =W_{q} \vx\,,                                                       \\
            K & =W_{k} \vx\,,                                                       \\
            V & =W_{v} \vx\,,                                                       \\
            A & =\operatorname{Softmax}\left(Q \cdot K^{T} / \sqrt{D / h}\right)\,, \\
            O & = W_{o} A V+b_{o}\,,
      \end{aligned}
      \label{sec:related_sa}
\end{equation}
%
\noindent $\vx$ are the $N$ patch tokens and the class token, of shape $(N, D)$, $D$ being the
embedding dimension. The patch tokens are linearly transformed thrice in a \textbf{Q}uery,
\textbf{K}ey, and \textbf{V}alue. An attention matrix of shape $(N, N)$ is computed from the query
and the key. Its $i^{\text{th}}$ row contains the similarity between the $i^{\text{th}}$ with all
other tokens. Finally, the multiplication between the attention matrix and the value matrix averages
all tokens according to their similarities. To extend the self-attention to its multi-heads
variation (\ac{MHSA}), we use several Query/Key/Value transformations and do as many self-attentions
in parallel.

\section{Continual Learning}
\label{sec:related_continual}

Usually, when training a \ac{CNN}, we assume the dataset is immutable and \textit{i.i.d.}: no new
images nor new classes will be learned. The knowledge acquired on one dataset \textit{A} can be
\textit{transferred} to another dataset \textit{B} with different classes using \textbf{transfer learning}
\citep{razavian2014transferlearning}. Then, the new model may be efficient on
the classes of dataset \textit{B} but cannot predict anymore the classes of dataset \textit{A}.

\textbf{Continual Learning} aims to learn a continually changing dataset without forgetting the
previous knowledge. The distribution of the dataset continually changes: \eg at each time-step, new
classes or new samples from potentially new domains are added to the training dataset
\citep{lomonaco2017core50}. We usually assume the test dataset evolves similarly. Multiple
distribution drifts exist in Continual Learning
\citep{morenotorresa2012datasetshift,lesort2021driftanalysis}, and they have been called under
various names in the literature. Given an input sample $x$ and its label $y$, the major drifts are:

\begin{itemize}
      \item \textbf{Covariate drift}: when $p(x)$ changes, it happens with the introduction of new
            domains \citep{volpi2021continualdomainadapt}
      \item \textbf{Prior drift}: when $p(y)$ changes; \ac{CIL} happens with this kind of drift.
      \item \textbf{Conceptual drift}: when $p(y | x)$ changes. Seldom covered in the literature, it
            can be found in \acf{CSS}.
\end{itemize}

Learning a ever-growing dataset is possible by training from scratch a new model on the union of
past and new data. However, for multiple reasons like privacy concerns of medical data or limited
storage capacity in embedded device \citep{vasquez2017incrementalneuralforest}, there is a
restriction on the amount of previous data that can be kept. In the extreme case, where a model only
has access to new data but not old data, a model trained from scratch cannot predict previous
iterations' distribution. Worse, even if the old model is kept and finetuned on the new data, it
will suffer from \textbf{Catastrophic Forgetting} \citep{robins1995catastrophicforgetting}: new data
are learned at the expanse of old data.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/protocol}
      \end{center}
      \caption{\textbf{Training protocol for incremental learning}. At each training task we learn a
            new set of classes, and the model must retain knowledge about \textit{all} classes. The
            model is allowed a \textit{limited} memory of samples of old classes.}
      \label{fig:related_protocol}
\end{figure}

\paragraph{Class-Incremental Example} More concretely, a common benchmark in \acf{CIL} is learning
the image classification CIFAR100 dataset \citep{krizhevskycifar100} in multiple steps, each made of
several new classes. \eg a model could learn at first to classify among 10 classes, then add 10 more
classes, \etc. Until it has learned all 100 classes. After each step, the model has to classify
among all classes it has learned. The \autoref{fig:related_protocol} illustrates such continual
protocol, and \autoref{fig:related_forgetting} depicts the performance of continual models in this
situation: the \textcolor{orange}{orange} line displays the accuracy of a model which is re-trained
from scratch at each step on all previous training data. This model, usually called Joint, is
considered as a reasonable upper bound. On the other hand, the \textcolor{blue}{blue} line is a
model finetuned solely on new classes without access to previous classes. The model's accuracy is
considerably lower than its Joint counterpart, because it forgets old classes by over-predicting new
classes. While we have mainly tackle \acf{CIL} benchmarks, we describe other continual benchmarks in
detail in the appendix (\autoref{sec:related_variation}).

\paragraph{Single-Head \vs Multi-Heads} are the two main evaluation settings in Continual Learning
\citep{chaudhry2018riemannien_walk}. In the former setting, a model has to classify samples among
all seen classes, that could have been learned from any of the seen steps. The latter setting knows
at test-time the step/task identifier of the samples. Thus, it only has to classify among the limited
number of classes brought by a step. This setting is closely related to multi-tasks learning. During
this thesis, we focus on the Single-Head setting because it is more realistic as it is not always
possible to know from which step a sample comes from in a real-life setting, and more challenging
\citep{lesort2019regulshortcomings}.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.8\linewidth]{images/related/catastrophic_forgetting.pdf}
      \end{center}
      \caption{\textbf{Training protocol for incremental learning}. At each training task we learn a
            new set of classes, and the model must retain knowledge about \textit{all} classes
            despite having no more access to old classes.}
      \label{fig:related_forgetting}
\end{figure}


%\subsection{Metrics for Continual Learning}
\label{sec:related_metrics}

\paragraph{Metrics} Multiple metrics exist in Continual Learning: the most common are the \textbf{final accuracy} and
\textbf{average incremental accuracy}. The former measures the performance of the model on all tasks
at the last step, the latter measures the average of performance on all seen tasks after each new
task learned \citep{rebuffi2017icarl}. Practically, given $A_{i,t}$ the accuracy of the $i^{th}$
task after
learning the $t^{th}$ task, the final accuracy is (assuming balanced tasks):
%
\begin{equation}
      \text{Acc}_F = \frac{1}{T} \sum_{i=1}^T A_{i,T}\,,
      \label{eq:related_final_acc}
\end{equation}
%
and the average incremental accuracy:
%
\begin{equation}
      \text{Acc}_a = \frac{1}{T} \sum_{t=1}^T \frac{1}{t}  \sum_{i=1}^t A_{i,t}\,.
      \label{eq:related_avg_acc}
\end{equation}
%
Average incremental accuracy is somewhat more important than simply the final accuracy: a continual
model should be good after every step because in a true continual setting, there is not a ``final
task''.

Other metrics exist \citep{diaz2018continualmetrics}, including the \textbf{backward and
      forward transfer} \citep{lopezpaz2017gem} that measures the influence that learning a task has on
the performance of respectively past and future tasks. Another notable metric is the
\textbf{forgetting} \citep{chaudhry2018riemannien_walk} which records how much a model has lost
performance-wise on a task compared to the first time it has learned it. The interest of this metric
is to be agnostic of the absolute performance of the model used.

Finally, metrics such as \textbf{speed} (\ie the number of images processed per second) or used
\textbf{capacity} (\ie number of learned parameters) are important:
\cite{ramasesh2022scalecontinual} recently showed that the larger a model was the lower was the
forgetting.

\section{Methods to reduce forgetting}
\label{sec:related_methods}

Multiple approaches exist to reduce forgetting in Continual Learning. The major ones are
rehearsal of old data (\autoref{sec:related_rehearsal}), regularizations constraining the model's
behavior (\autoref{sec:related_regul}), and structural adaptations (\autoref{sec:related_structural}).

\subsection{Rehearsal}
\label{sec:related_rehearsal}

The most efficient method to reduce forgetting is \textbf{rehearsal learning} where old samples will
be seen alongside the new samples. The amount of old samples stored is extremely limited, otherwise,
it would defeat the purpose of continual learning. \autoref{fig:related_protocol_rehearsal} illustrates how
rehearsal learning happens in Continual Learning. During the first step, a model is trained on all
available samples. Then, it stores a limited amount of those in a \textit{memory}. During the second
step, the model has access to new samples but also all samples stored in the memory. In
Class-Incremental, an equal amount of samples per class is stored in memory. There are
two major approaches to determine this amount: \cite{rebuffi2017icarl} propose to fully use a
memory of size $\mcM$ among all $\mcC$, while \cite{hou2019ucir} instead kept fixed the number of
samples stored per class to $\nicefrac{\mcM}{|\mcC|}$.

\paragraph{Herding} is the action of choosing which samples per class to store in the rehearsal
memory. The most naive herding method is to randomly sample images. Despite its simplicity, it is
quite competitive with more complex method \citep{castro2018end_to_end_inc_learn}, echoing similar
results in \ac{AL} \citep{gal2017activelearning}. Other herding methods include fetching samples
close to the class mean in the feature space \citep{castro2018end_to_end_inc_learn} or close to an
incremental barycenter \citep{rebuffi2017icarl}.

\paragraph{Sampling} is an important but yet fewly investigated topic in Continual Learning. Most
models mix all memory samples with new samples without any under- or over-sampling.
\cite{castro2018end_to_end_inc_learn} propose to finetune for a few epochs, after training on a new
step, on a balanced set of old and new classes samples. \cite{chaudhry2019tinyepisodicmemories}
oversample tiny memory with as low as one sample per class, and show, in the context of Online
Learning where models learn in only one epoch, that continual models still do not overfit. In the
same context, \cite{aljundi2019maximallyinterfered} propose to over-sample the memory examples with
the highest losses. In an imbalanced situation for Continual Learning, over- and under-sampling can be
applied depending on the number of samples per class \citep{kim2020imbalancedcontinual}.

\paragraph{Efficient Storing} is important for rehearsal learning: a bigger rehearsal memory leads
invariably to less forgetting \citep{douillard2020podnet}. Thus, several works considered how more
samples could be stored given the same memory size: \cite{hayes2020remind} compress intermediate
features of memory samples with a lossless compression algorithm.
\cite{iscen2020incrementalfeatureadaptation} also store features but modify them through the
training to handle the inherent internal covariate drift. \cite{douillard2021objectrehearsal}, in
the context of segmentation, store only non-regular patches of zones of interest instead of storing
the whole large images.

\paragraph{Pseudo-rehearsal} does not need to store samples, but instead generates pseudo-samples for
rehearsal \citep{lesort2019generative}. The generation can be done with auto-encoders from
intermediate features \citep{kemker2018fearnet,ayub2021eec} or use \ac{GAN}
\citep{shin2017deep_generative_replay}. Those methods unfortunately have several drawbacks: they
struggle to scale to large images, the generator size may be superior to a classic rehearsal memory
size which would defeat the goal of using less storage, and finally the generator may itself suffer
from catastrophic forgetting. \cite{liu2020mnemonics} propose instead a method halfway between
rehearsal and pseudo-rehearsal: the authors sample randomly real images, and then during continual
training, slightly modify them via bi-level optimization to minimize forgetting.


\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/rehearsal}
      \end{center}
      \caption{\textbf{Training with a rehearsal memory}. After each task a fraction of the
            data is stored in a memory to be used in the next task. Rehearsal learning is the most
            efficient method to reduce forgetting, but unfortunately the memory capacity is often
            extremely limited.}
      \label{fig:related_protocol_rehearsal}
\end{figure}

\subsection{Regularization-based Approaches}
\label{sec:related_regul}


\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/constraints}
      \end{center}
      \caption{\textbf{Constraining the new model based on the old model}. During each task, after
            the first one, the new model is constrained to be similar to the old model in order to reduce
            forgetting.}
      \label{fig:related_protocol_constraints}
\end{figure}

A common and efficient way to reduce forgetting, is to minimize the difference in behavior between
the old and new models as illustrated in \autoref{fig:related_protocol_constraints}. These constraints can be
expressed through various forms and are described below.

\subsubsection{Weight-based}
\label{sec:related_regul_weight}


The most straightforward way to avoid completely forgetting, is that the old and new
models stay identical. While the model would be \textit{rigid} (no forgetting), it is also not
\textit{plastic} at all, and thus cannot learn any new tasks. Thus, a line of research proposed to
constrain only a portion of the neurons:
%
\begin{equation}
      \mcL(\theta) = \mcL_\mcT(\theta) + \lambda \sum_i \Omega_i (\theta_i - \theta_i^*)^2\,,
      \label{eq:related_weight_constraint}
\end{equation}
%
where $\mcL_\mcT(\theta)$ is the loss at the current task (\eg the cross-entropy), $\theta_i$ and
$\theta_i^*$ respectively the $i^\text{th}$ neuron of the current and previous model, and $\Omega_i$
a neuron-wise importance factor. The intuition is that important neuron for the previous task $\mcT-1$ should not
change, while the others can be adapted to fit the new task $\mcT$.

\cite{kirkpatrick2017ewc}, followed by
\cite{zenke2017synaptic_intelligence} and \cite{chaudhry2018riemannien_walk}
propose to use the diagonal Fisher information matrix as importance factors.
\cite{aljundi2018MemoryAwareSynapses} instead used the sensitivity of the model when
small perturbations are added to the neurons to measure their importance.

However, it is worth remarking that weight-based constraints are usually limited to the multi-heads
setting where a task identifier is available at test time. \cite{lesort2019regulshortcomings} showed
that in the more realistic single-head setting, they barely reduce forgetting and were significantly
outperformed by rehearsal-based methods.

\subsubsection{Gradient-Based}
\label{sec:related_regul_gradient}

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/gem.pdf}
      \end{center}
      \caption{\textbf{GEM's gradient constraint} forcing updates to be in the same direction as the
            gradient \wrt old samples.}
      \label{fig:related_gem}
\end{figure}

\cite{lopezpaz2017gem} propose the GEM model that combines a constraint on the gradients and
rehearsal learning. The algorithm requires that the loss on a given stored sample must not increase
despite the model learning new classes. The authors, given a locality assumption, rewrite this
formulation as enforcing that the gradient of a new sample ($g$) to be in the same \textit{direction} as
the gradient of a stored old sample ($g_i$):
%
\begin{equation}
      \langle g,\, g_i\rangle \ge 0,\, \text{for all}\, i \in \mcM\,,
\end{equation}
%
\noindent with $\mcM$ the rehearsal memory. If the constraint is violated, the new gradient $g$ is
projected to the closest in L2 norm gradient that satisfies the angle constraint by minimizing a
quadratic program. The constraint is illustrated in \autoref{fig:related_gem}. The drawback of this
method is the computational cost that can grow prohibitively when the memory is too large.
\cite{chaudhry2019AGEM} propose Averaged-GEM to speed up GEM: the authors do not constraint the
gradient of individual memory samples but only the average of all memory samples.
\cite{aljundi2019gradientselection} also improved GEM's speed by selecting only a subset of the
memory samples that maximize the feasible region.

Differently, but still constraining the gradients: \cite{farajtabar2020ogd}'s OGD forces the gradients of
task $t$ to be orthogonal to gradients of task $t-1$. They use the Gram-Schmidt procedure to
orthogonalize the new gradients, allowing updates for the new task that minimally interfere with the
performance of old tasks. \cite{saha2021gpm}'s GPM does likewise but uses instead a k-rank
approximation of the SVD of the representation matrix.


\subsubsection{Output-based}
\label{sec:related_regul_output}

Finally, the majority of Continual models that are benchmarked on large datasets (\eg ImageNet
\citep{deng2009imagenet}, Pascal VOC \citep{everingham2015pascalvoc}) use a combination of rehearsal
learning (\autoref{sec:related_rehearsal}) and constraints on the model's outputs.

LwF \cite{li2018lwf} and iCaRL apply the \ac{KD} \cite{hinton2015knowledge_distillation} on the
model's probabilities. It usually consists in minimizing the \ac{KL} between the probabilities
of the old and new models:
%
\begin{equation}
      \mcL_\text{KD} = \operatorname{KL}(\operatorname{softmax}(\frac{\tilde{y}^{t-1}}{\tau}) \Vert \operatorname{softmax}(\frac{\tilde{y}^t}{\tau})\,,
\end{equation}
%
where $\tilde{y}^{t-1}$ and $\tilde{y}^{t}$ are respectively the logits of the old and new model,
and $\tau$ a \textit{temperature} to soften the probabilities in order to give more importance to
the \textit{dark knowledge} of \cite{hinton2015knowledge_distillation}. Note that in the context of
Class-Incremental, the new model predicts more classes than the old model, therefore, the \ac{KL} is
only applied on the logits common to both the old and the new models. The \ac{KD} is sometimes also
defined as the binary cross-entropy between the sigmoid-activated logits.

Constraining the probabilities is now so ubiquitous that most models include it in their base
losses. On the other hand, a few models considered constraining intermediate outputs. MK2D
\citep{peng2019m2kd} uses the \ac{KD} from both the final classifier and an auxiliary classifier
similarly to the Inception network \citep{szegedy2015inception}. \cite{hou2019ucir} maximizes the
cosine similarity between the embeddings produced by the \ac{GAP}.
\cite{dhar2019learning_without_memorizing_gradcam} minimizes the L1 distance between the attention
maps produced by GradCam \citep{selvaraju2017gradcam}. Drawing inspiration from the model
compression literature \cite{zagoruyko2016distillation_attention}, \cite{douillard2020podnet}
propose to instead constraint the statistics of all intermediate features of the \ac{ConvNet}.
\cite{douillard2020plop} expand this constraint to incorporate multiscale information.

\subsection{Structural Strategies}
\label{sec:related_structural}

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.6\linewidth]{images/related/subnetworks.pdf}
      \end{center}
      \caption{\textbf{Task-specific subnetworks} that can be uncovered with a sparsity loss or
            learned masking.}
      \label{fig:related_subnetwork}
\end{figure}

Multiple works have also propose to adopt dynamic strategies where the configuration of the neural
network is different for each task to learn.

\paragraph{Subnetworks} The Lottery Ticket Hypothesis \citep{frankle2019lottery_ticket} states that
subnetworks, made of a fraction of the neurons and connections of a larger network, can reach
excellent performance. Several Continual Learning models exploit that property by using a
subnetwork per task. Those subnetworks can be uncovered via genetic algorithms
\citep{fernando2017path_net}, via induced L1 sparsity \citep{golkar2019neural_pruning}, or even
learned masked \cite{serra2018hat,hung2019cpg}. Usually, these methods require a task identifier at
test-time in order to select the right subnetwork (see \textit{multi-heads} in
\autoref{sec:related_continual}). Later, \cite{wortsman2020supermasks} propose to infer the task
identifier by selecting the subnetwork with the lowest entropy. This subnetwork-based approach is
illustrated in \autoref{fig:related_subnetwork}.

\paragraph{Expandable Networks} A neural network can also be expanded through its continual
training to accommodate the growing amount of tasks to solve. First, \cite{rusu2016progressive}
propose to have one network per task, where the $i^{\text{th}}$ network would depend both on the
input and all previous networks' intermediate features. Unfortunately, the memory consumption is
quickly prohibitive with many tasks. Following works propose to only add blocks of parameters,
and only when deemed necessary
\citep{hung2019cpg,veniat2021mntdp,ostapenko2021localmodulecomposition}.

\paragraph{Task Conditioning} Rather than adding many new parameters, it is also possible to only add
a few parameters that will adapt the existing network \citep{rebuffi2017visualadapters}:
\cite{wen2020batchensemble} and \cite{sun2019metatransfer} propose to share most of the weights
across tasks, but have task-specific weights that directly modify the shared weights.

\paragraph{Mixture-of-Experts} Mixture of experts \citep{masoudnia2014mixture} have also been
proposed, where multiple experts combine their decision. \cite{aljundi2017experts} learn a gating
system to use the right task-specific expert. \cite{yan2021der} and \cite{li2021preserve} proposed
to have a \ac{ConvNet} per task and to concatenate their output embeddings to be fed to a single
classifier. To avoid parameters explosion, they aggressively prune each \ac{ConvNet}.

\paragraph{Classifier Correction} Forgetting happens in both the feature extractor and the
classifier. Previously described rehearsal and regularization methods try to reduce it in both
places. On the other hand, multiple works focus solely on the classifier. They remark that in
\acf{CIL}, the classifier is miscalibrated \citep{guo2017miscalibration} where the model
over-predicts new classes to the detriment of old classes. \cite{belouadah2019il2m} compensate the
bias towards new classes by rectifying predictions of past classes using their recorded accuracies
and confidences. \cite{wu2019bias_correction} learn a linear model on validation data to recalibrate
the logits of the new classes. \cite{zhao2020weightalignement} normalizes the norm of the classifier
weights associated with new classes so that their average norm becomes the same as that for old
classes. \cite{hou2019ucir}, aims for a similar result by replacing the dot product in the
classifier by the cosine similarity, resulting in unit norm classifier weights.

\section{Positioning}

Continual Learning encompasses very different benchmarks and methods. In this thesis, we tackle
multiple types of continual drift using different approaches summarized below:

\paragraph{Feature-based Regularizations} First in \autoref{chapter:regularization}, we consider
\acf{CIL} scenarios with a prior drift where new classes are continually added. This is a
challenging benchmark, even more when tackling a large amount of classes with datasets like CIFAR100
(100 classes) and ImageNet (1000 classes). The previous State-of-the-Art models all use a
combination of rehearsal (\autoref{sec:related_rehearsal}) and regularizations of the predicted
probabilities (\autoref{sec:related_regul_output}). In this chapter, we claim that this type of
constraints struggle to efficiently balance the rigidity needed to not forget with the plasticity
required to learn new classes. Therefore, we propose to regularize, not the final outputs, but
rather the intermediate feature space. In the first section (\autoref{sec:podnet}), we describe how
we regularize statistics on the features at multiple levels of the \ac{ConvNet} with our method
called POD. The choice of which statistics to employ has an important impact on forgetting, which I
ablate extensively. While this first section aims to constrain the feature space to reduce
forgetting, the second section (\autoref{sec:ghost}) tries to avoid forgetting before it even
happen: Inspired by \ac{ZSL}, we estimate the feature representation of future classes using weak
information about them (\eg attributes). Exclusion zones in the feature space are created for those
yet unseen classes, so that when finally learned, the confusion between old and new classes is
minimal.

\paragraph{Continual Semantic Segmentation} Second, in \autoref{chapter:segmentation}, we tackle
\acf{CSS}. In this benchmark, images have a label per pixel, and only the current classes are
labelized. Thus, both the prior and concept drifts happen where new classes are added but also the
signification of a pixel can change through time. Three different, but complementary, methods are
used for this type of data: first we use an uncertainty-based pseudo-labeling to handle what we
called a background shift. Then, we extend the feature regularization
(\autoref{sec:related_regul_output}) introduced in the previous chapter, POD, to constrain
differently local and global areas of the spatial feature maps. Finally, we consider rehearsal
learning (\autoref{sec:related_rehearsal}) propose a new rehearsal, designed for segmentation, by
exploiting a copy-paste of masks resulting in State-of-the-Art performance while being very memory
efficient.

\paragraph{Dynamic Transformers} In this third and last chapter (\autoref{chapter:dynamic}), we only
aim to solve the prior drift but using a method radically different from previous chapters: dynamic
networks (\autoref{sec:related_structural}). Previous dynamic networks usually expand their
capacity by a large margin during the continual training to handle the growing amount of tasks to
learn. To avoid a parameter count explosion, the models are usually pruned aggressively. The main
drawback of these methods is that the pruning can still result in models too large and often need
careful finetuning of hyperparameters. We propose in this chapter, a novel dynamic expansion with
almost no memory overhead contrary to concurrent works. Based on the transformer architecture, a
special learned vector is used per task to specialize the features to each task. Consequently,
forgetting is greatly reduced while our memory and time overheads stay manageable.
