\chapter{Related Works}
\label{chapter:related}

%\minitoc
\chapterwithfigures{\nameref*{chapter:related}}
\chapterwithtables{\nameref*{chapter:related}}

\ifthenelse{\boolean{skipRelated}}{\endinput}{}

\section{Deep Learning for Computer Vision}

\acf{CNN} have been for a long-time the \textit{defacto} method for \acs{NN} for \ac{CV}
\citep{fukushima1980neocognitron,lecun1999lenet}. In 2012, with larger datasets and better hardware
(\ie \acs{GPU}), \cite{krizhevsky2012alexnet} imposed \ac{DL} models as the best performing
algorithms for images. As the name implies, \acs{CNN} are made of convolutions. The convolution
operation involves sliding a kernel across the image pixels:

\begin{equation}
    \resizebox{\hsize}{!}{$
      \left(\left[\begin{array}{lll}
                  a & b & c \\
                  d & e & f \\
                  g & h & i
            \end{array}\right] *\left[\begin{array}{lll}
                  1 & 2 & 3 \\
                  4 & 5 & 6 \\
                  7 & 8 & 9
            \end{array}\right]\right)[2,2]=(a \cdot 1)+(b \cdot 2)+(c \cdot 3)+(d \cdot 4)+(e \cdot 5)+(f \cdot 6)+(g \cdot 7)+(h \cdot 8)+(i \cdot 9)\,,
        $}
      \label{eq:related_conv}
\end{equation}

\noindent where the left matrix is a $3\times 3$ convolution kernel, and the right matrix a $3
      \times 3$ grayscale (\ie single-channel) image. Note that while the literature commonly call
this a "\textit{convolution}", it's actually a "\textit{cross-correlation}" because the kernel
hasn't been flipped. If an image contains multiple channels, as an RGB image, a different kernel
is applied to each channel, and the resulting matrices are summed element-wise. \textbf{The
      convolution allows extracting patterns in an image}. Before Deep Learning, convolution kernels
were hand-made, likewise:

\begin{equation}
      M_{x}=\frac{1}{4}\left[\begin{array}{lll}
                  -1 & 0 & 1 \\
                  -2 & 0 & 2 \\
                  -1 & 0 & 1
            \end{array}\right]\,.
      \label{eq:sobel_horizontal}
\end{equation}

This kernel is the horizontal Sobel kernel which can detect horizontal edges. Detecting more complex
patterns means more complex kernels, which would be difficult to design manually. Therefore,
\acs{CNN} considers each element of the kernel as a neuron to be optimized (see
\autoref{chapter:introduction}). Combining convolution kernels successively allows detecting
more and more complex patterns until being able to detect object concepts in an image. In
\autoref{fig:related_cnn}, I illustrate a \ac{CNN} made of convolutions, poolings (a
dimensionality-reduction/sampling method for images), and a final classifier. The classifier is simply
a linear projection from the features dimension to the number of classes, where with a softmax
activation, logits will be mapped to probabilities.


\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=\linewidth]{images/related/cnn.pdf}
      \end{center}
      \caption{\textbf{A Convolutional Neural Networks} extracts more complex patterns through its
            succession of convolutions. In \textcolor{orange}{orange} a convolution, in \textcolor{red}{red}
            a pooling, and in \textcolor{green}{the classifier}. Given an image, the \ac{CNN} can assign to
            each possible class a probability, all summing to 1. Here this cute little cat is clearly a cat.
            Detected patterns taken from \cite{olah2017feature}.}
      \label{fig:related_cnn}
\end{figure}


In the 2010's decade, multiple major improvements were made to \acs{CNN}, but the one of
the latest best performing models \citep{tan2019efficientnet} is mainly based on two architectural changes:
residual connections allowing training of very deep networks \citep{he2016resnet} and normalization
of intermediate activations \citep{ioffe2015batchnorm}. Another major factor of recent \ac{CNN}
improvements is better training procedure \citep{wightman2019resnetstrikesback}, which includes
optimizer \citep{kingma2014adam}, learning rate scheduling, strong data augmentations
\citep{muller2021trivialaugment,hingyi2018mixup,zhong2017erasing}, and regularizations
\citep{gal2016dropout,gao2016stochasticdepth}.

More recently, the Transformer architecture \citep{vaswani2017transformer}, originally designed for
machine translation in \ac{NLP}, was applied to images. Using an encoder structure similar to BERT
\citep{devlin2018bert}, ViT \citep{dosovitskiy2020vit} considered patches of pixels as tokens.

\section{Continual Learning}
\label{sec:related_continual}

Usually, when training a \ac{CNN}, we assume the dataset is immutable and \textit{i.i.d.}: no new
image nor new classes will be learned. The knowledge acquired on one dataset A can be
\textit{transferred} to another dataset B with different classes using \textbf{transfer learning}
\citep{razavian2014transferlearning}. However, in that case, the new model, while being efficient on
the dataset B, cannot classify the classes of dataset A.

\textbf{Continual Learning} aims to learn a continually changing dataset without forgetting the
previous knowledge. The distribution of the dataset continually change: at each time-step, \ac{NC},
\ac{NI} from potentially new domains, or even \ac{NIC} are added to the training dataset
\cite{lomonaco2017core50}. We usually assume the test dataset evolves similarly. Continually
learning an ever-growing dataset is doable: a naive but efficient approach consist in training from
scratch a new model on the union of past and new data. However, for multiple reasons like privacy
concerns of medical data or limited storage capacity in embedded device, there is a restriction on
the amount of previous data that can be kept. In the extreme case, where a model only has access to
new data but now old data, training from scratch fails to model previous iterations' distribution.
Worse, even if the old model is kept and finetuned on the new data, it'll suffer from
\textbf{Catastrophic Forgetting} \citep{robins1995catastrophicforgetting}: new data is learned at
the expanse of old data. Multiple distribution shifts exist in Continual Learning
\citep{morenotorresa2012datasetshift,lesort2021driftanalysis}, and they have been called under various names in the
literature. I detail now the major ones, given $x$ an input sample and $y$ its label:

\begin{itemize}
      \item \textbf{Covariate shift}: when $p(x)$ changes, also known as domain incremental.
      \item \textbf{Prior shift}: when $p(y)$ changes; \ac{CIL} happens with this kind of shift.
      \item \textbf{Conceptual shift}: when $p(y | x)$ changes. Seldom covered in the literature, it
            can be found in \acf{CSS}.
\end{itemize}

In the chapter \autoref{chapter:regularization} and \autoref{chapter:dynamic}, I tackled only the
prior shift, while in \autoref{chapter:segmentation} I solved all three described shifts.


\paragraph{Class-Incremental Example} More concretely, a common benchmark in \ac{NC} setting,
commonly called \ac{CIL} is to learn the image classification CIFAR100 dataset
\citep{krizhevskycifar100} in multiple steps, each made of several new classes. \eg a model could
learn at first to classify among 10 classes, then add 10 more classes, \etc. until it has learned
all 100 classes. After each step, the model has to classify among all classes it has learned. The
\autoref{fig:related_forgetting} illustrate such continual training. The \textcolor{orange}{orange}
line displays the accuracy of a model that is re-trained from scratch at each step on all previous
classes data. This model, usually called Joint, is considered as a reasonable upper bound. The
\textcolor{blue}{blue} line on the other hand is a model that is finetuned on new classes but has no
access to previous classes. Evidently, the model's accuracy is much lower than its Joint
counterpart, because it forgets completely old classes by over-predicting new classes.

\paragraph{Single-Head \vs Multi-Heads} are the two main evaluation settings in Continual Learning
\citep{chaudhry2018riemannien_walk}. In the former setting, a model has to classify samples among
all seen classes, that could have been learned from any of the seen steps. The latter setting, on
the other hand, knows at test-time from which steps the samples come from. Thus, it only has to
classify among the limited number of classes brought by a step. This setting is closely related to
multi-tasks learning. During this thesis, I focused on the Single-Head setting because it's more
realistic as it's not always possible to know from which step a sample come from in a real-life
setting, and more challenging \citep{lesort2019regulshortcomings}.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.8\linewidth]{images/related/catastrophic_forgetting.pdf}
      \end{center}
      \caption{\textbf{Training protocol for incremental learning}. At each training task we learn a
            new set of classes, and the model must retain knowledge about \textit{all} classes. The
            model is allowed a \textit{limited} memory of samples of old classes.}
      \label{fig:related_forgetting}
\end{figure}

\paragraph{Notations} I define in \autoref{tab:related_notation} the notations used thorough this thesis.

\input{tables/related/notations.tex}

\section{Metrics for Continual Learning}
\label{sec:related_metrics}

\epigraph{If you cannot measure it, you cannot improve it.}{\textit{Lord Kelvin}}

Multiple metrics exist in Continual Learning: the most common are the \textbf{final accuracy} and
\textbf{average incremental accuracy}. The former measures the performance of the model on all tasks
at the last step. The latter measures the average of performance on all seen tasks after each new
task learned \citep{rebuffi2017icarl}. Practically, given $A_{i,t}$ the accuracy of the $i^{th}$
task after
learning the $t^{th}$ task, the final accuracy is (assuming balanced tasks):
%
\begin{equation}
      \text{Acc}_F = \frac{1}{T} \sum_{i=1}^T A_{i,T}\,,
      \label{eq:related_final_acc}
\end{equation}
%
and the average incremental accuracy:
%
\begin{equation}
      \text{Acc}_a = \frac{1}{T} \sum_{t=1}^T \frac{1}{t}  \sum_{i=1}^t A_{i,t}\,.
      \label{eq:related_avg_acc}
\end{equation}
%
Average incremental accuracy is somewhat more important than simply the final accuracy: a continual
model should be good after every step, as in a true continual setting there are no "final task".

Multiple other metrics exists \citep{diaz2018continualmetrics}, including the \textbf{backwardø and forward transfer} \citep{lopezpaz2017gem}
that measures the influence that learning a task has on the performance of respectively past and
future tasks. Another notable metric is the \textbf{forgetting} \citep{chaudhry2018riemannien_walk}
which records how much a model has lost performance-wise on a task compared to the first time it has
learned it. The interest of this metric is to be agnostic of the absolute performance of the model used.

Finally, metrics such as \textbf{speed} or used \textbf{capacity} are important: \cite{ramasesh2022scalecontinual}
recently showed that the larger a model was the lower was the forgetting.

\section{Methods to reduce forgetting}
\label{sec:related_methods}

\subsection{Rehearsal}
\label{sec:related_rehearsal}

Multiple approaches exist to reduce catastrophic forgetting. The most efficient method is
\textbf{rehearsal learning} where old samples will be seen alongside the new samples. The amount of
old samples stored is extremely limited otherwise it would defect the purpose of continual learning.
\autoref{fig:related_protocol} illustrates how rehearsal learning happens in Continual Learning.
During the first step, a model is trained on all available samples. Then, it stores a limited amount
of those in a \textit{memory}. During the second step, the model has access to new samples but also
all samples stored in the memory. In Class-Incremental, an equal amount of samples per class is
stored in memory. Nevertheless, there are two major approaches to determine this amount:
\cite{rebuffi2017icarl} proposed to fully use a memory of size $\mcM$ among all $\mcC$, while
\cite{hou2019ucir} instead kept fixed the amount of samples stored per class to $\nicefrac{\mcM}{|\mcC|}$.

\paragraph{Herding} is the action of choosing which samples per class to store in the rehearsal
memory. The most naive herding method is to sample randomly images. Despite its simplicity, it's
quite competitive with more complex method \citep{castro2018end_to_end_inc_learn}, echoing similar
results in \ac{AL} \citep{gal2017activelearning}. Other herding methods include fetching samples
close to the class mean in the feature space \citep{castro2018end_to_end_inc_learn} or close to an
incremental barycenter \citep{rebuffi2017icarl}.

\paragraph{Sampling} is an important but yet fewly investigated topic in Continual Learning. Most
models mix all memory samples with new samples without any under- or over-sampling.
\cite{castro2018end_to_end_inc_learn} propose to finetune for a few epochs, after training on a new
step, on a balanced set of old and new classes samples. \cite{chaudhry2019tinyepisodicmemories}
oversample tiny memory with as low as one sample per class, and show, in the context of Online
Learning (\autoref{sec:related_online_learning}), that continual models still don't overfit. In the
same context, \cite{aljundi2019maximallyinterfered} proposed to over-sample the memory examples with
highest losses. In an imbalanced situation for Continual Learning, over- and under-sampling can be
applied depending on the amount of samples per classes \citep{kim2020imbalancedcontinual}.

\paragraph{Efficient Storing} is important for rehearsal learning: a bigger rehearsal memory leads
invariably to less forgetting \citep{douillard2020podnet}. Thus, several works considered how more
samples could be stored given the same memory size: \cite{hayes2020remind} compress intermediate
features of memory samples with a lossless compression algorithm.
\cite{iscen2020incrementalfeatureadaptation} also store features but modify them through the
training to handle the inherent internal covariate shift. \cite{douillard2021objectrehearsal}, in
the context of segmentation, store only non-regular patches of zones of interest instead of storing
the whole, large, images.

\paragraph{Pseudo-rehearsal} doesn't need to store samples, but instead generates pseudo-samples for
rehearsal \citep{lesort2019generative}. The generation can be done with auto-encoders from
intermediate features \citep{kemker2018fearnet,ayub2021eec} or use \ac{GAN}
\citep{shin2017deep_generative_replay}. Those methods unfortunately have several drawbacks: they
struggle to scale to large images, the generator size may be superior to a classic rehearsal memory
size which would defeat the goal of using less storage, and finally the generator may itself suffer
from catastrophic forgetting. \cite{liu2020mnemonics} propose instead a method halfway between
rehearsal and pseudo-rehearsal: the authors sample randomly real images, and then during continual
training, slightly modify them via bi-level optimization to minimize forgetting.


\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/podnet/protocol}
      \end{center}
      \caption{\textbf{Training protocol for incremental learning}. At each training task we learn a
            new set of classes, and the model must retain knowledge about \textit{all} classes. The
            model is allowed a \textit{limited} memory of samples of old classes.}
      \label{fig:related_protocol}
\end{figure}

\autoref{fig:related_protocol}


\subsection{Regularization-based Approaches}
\label{sec:related_regul}


\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/continual_regularizations.pdf}
      \end{center}
      \caption{\textbf{Regularizations constraints} forcing the new model to be \textit{similar} to
            the old model to reduce catastrophic forgetting.}
      \label{fig:related_regul}
\end{figure}

A common and efficient way to reduce forgetting, is to minimize the difference in behavior between
the old and new models as illustrated in \autoref{fig:related_regul}. These constraints can be
expressed through various forms as described below.

\subsubsection{Weight-based}
\label{sec:related_regul_weight}


The most straightforward way to avoid completely forgetting, is that the old and new
models are identical. While the model would be \textit{rigid} (no forgetting), it is also not
\textit{plastic} at all, and thus cannot learn any new tasks. Thus, a line of research proposed to
constrain only a portion of the neurons:
%
\begin{equation}
      \mcL(\theta) = \mcL_\mcT(\theta) + \lambda \sum_i \Omega_i (\theta_i - \theta_i^*)^2\,,
      \label{eq:related_weight_constraint}
\end{equation}
%
where $\mcL_\mcT(\theta)$ is the loss of the current task (\eg the cross-entropy), $\theta_i$ and
$\theta_i^*$ respectively the $i^\text{th}$ neuron of the current and previous model, and $\Omega_i$
a neuron-wise importance factor. The intuition is that important neuron for $\mcT-1$ shouldn't
change, while the others can be adapted to fit the new task $\mcT$.

\cite{kirkpatrick2017ewc}, followed by
\cite{zenke2017synaptic_intelligence} and \cite{chaudhry2018riemannien_walk}
proposed to use the diagonal Fisher information matrix as importance factors.
\cite{aljundi2018MemoryAwareSynapses} instead used the sensitivity of the model when
small perturbations are added to the neurons to measure their importance.

However, it's worth remarking that weight-based constraints are usually limited to the multi-heads
setting where a task identifier is available at test time. \cite{lesort2019regulshortcomings} showed
that in the more realistic single-head setting, they barely reduce forgetting and were significantly
outperformed by rehearsal-based methods.

\subsubsection{Gradient-Based}
\label{sec:related_regul_gradient}

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=1.0\linewidth]{images/related/gem.pdf}
      \end{center}
      \caption{\textbf{GEM's gradient constraint} forcing updates to be in the same direction as the
            gradient \wrt old samples.}
      \label{fig:related_gem}
\end{figure}

\cite{lopezpaz2017gem} proposed the GEM model which combines a constraint on the gradients and
rehearsal learning. The algorithm requires that the loss on a given stored sample must not increase
despite the model learning new classes. The authors, given a locality assumption, rewrite this
formulation as enforcing that the gradient of a new sample ($g$) to be in the same \textit{direction} as
the gradient of a stored old sample ($g_i$):
%
\begin{equation}
      \langle g,\, g_i\rangle \ge 0,\, \text{for all} i \in \mcM
\end{equation}
%
With $\mcM$ the rehearsal memory. If the constraint is violated, the new gradient $g$ is projected
to the closest in L2 norm gradient that satisfies the angle constraint by minimizing a quadratic
program. The constraint is illustrated in \autoref{fig:related_gem}. The drawback of this method is
the computational cost that can grow prohibitively when the memory is too large.
\cite{chaudhry2019AGEM} proposed Averaged-GEM to speed up GEM: the author don't constraint the
gradient of individual memory samples but only the average of all memory samples.
\cite{aljundi2019gradientselection} also improved GEM's speed by selecting only a subset of the
memory samples that maximize the feasible region.

Differently, but still constraining the gradients: \cite{farajtabar2020ogd}'s OGD forces the gradients of
task $t$ to be orthogonal to gradients of task $t-1$. They use the Gram-Schmidt procedure to
orthogonalize the new gradients, allowing updates for the new task that minimally interfere with the
performance of old tasks. \cite{saha2021gpm}'s GPM does likewise but using instead a k-rank
approximation of the SVD of the representation matrix.


\subsubsection{Output-based}
\label{sec:related_regul_output}

Finally, the majority of Continual model that are benchmarked on large datasets (\eg ImageNet
\citep{deng2009imagenet}, Pascal VOC \citep{everingham2015pascalvoc}) use a combination of rehearsal
learning (\autoref{sec:related_rehearsal}) and constraints on the model's outputs.

LwF \cite{li2018lwf} and iCaRL applied the \ac{KD} \cite{hinton2015knowledge_distillation} on the
model's probabilities. It usually consists in minimizing the \ac{KL} between the probabilities
of the old and new models:
%
\begin{equation}
      \mcL_\text{KD} = \operatorname{KL}(\operatorname{softmax}(\frac{\tilde{y}^{t-1}}{\tau}) \Vert \operatorname{softmax}(\frac{\tilde{y}^t}{\tau})\,,
\end{equation}
%
where $\tilde{y}^{t-1}$ and $\tilde{y}^{t}$ are respectively the logits of the old and new model,
and $\tau$ a \textit{temperature} to soften the probabilities in order to give more importance to
the \textit{dark knowledge} of \cite{hinton2015knowledge_distillation}. Note that in the context of
Class-Incremental, the new model predicts more classes than the old model, therefore the \ac{KL} is
only applied on the logits common to both the old and the new models. The \ac{KD} is sometimes also
defined as the binary cross-entropy between the sigmoid-activated logits.

Constraining the probabilities is now so ubiquitous that most models include it in their base
losses. On the other hand, a few models considered constraining intermediate outputs. MK2D
\citep{peng2019m2kd} uses the \ac{KD} from both the final classifier and an auxilliary classifier
similarly to the Inception network \citep{szegedy2015inception}. \cite{hou2019ucir} maximizes the
cosine similarity between the embeddings produced by the \ac{GAP}.
\cite{dhar2019learning_without_memorizing_gradcam} minimizes the L1 distance between the attention
maps produced by GradCam \citep{selvaraju2017gradcam}. Drawing inspiration from the model
compression litterature \cite{zagoruyko2016distillation_attention}, \cite{douillard2020podnet}
proposed to constraint instead the statistics of all intermediate features of the \ac{ConvNet}. This
constraint was later expanded to incorporate multiscale information \cite{douillard2020plop}.

\subsection{Dynamic Strategies}
\label{sec:related_dynamic}

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.6\linewidth]{images/related/subnetworks.pdf}
      \end{center}
      \caption{\textbf{Task-specific subnetworks} that can be uncovered with a sparsity loss or
            learned masking.}
      \label{fig:related_subnetwork}
\end{figure}

Multiple works have also proposed to adopt dynamic strategies where the configuration of the neural
network is different for each task to learn.

The Lottery Ticket Hypothesis \citep{frankle2019lottery_ticket} states that subnetworks, made of a
fraction of the neurons and connections of a larger network, can reach excellent performance.
Several Continual Learning models have exploited that property by using a subnetwork per task. Those
subnetworks can be uncovered via genetic algorithms \citep{fernando2017path_net}, via induced L1
sparsity \citep{golkar2019neural_pruning}, or even learned masked \cite{serra2018hat,hung2019cpg}.
Usually, these methods require a task identifier at test-time in order to select the right
subnetwork (see \textit{multi-heads} in \autoref{sec:related_continual}). Later,
\cite{wortsman2020supermasks} proposed to infer the task identifier by selecting the subnetwork with
the lowest entropy. This subnetwork-based approach is illustrated in \autoref{fig:related_subnetwork}.

A neural network can also be expanded thorough its continual training to accommodate the growing
amount of tasks to solve. First, \cite{rusu2016progressive} proposed to have one network per task,
where the $i^{\text{th}}$ network would depend both on the input and all previous networks'
intermediate features. Unfortunately, the memory consumption is quickly prohibitive with many tasks.
Following works have proposed to only add blocks of parameters, and only when deemed necessary
\citep{hung2019cpg,veniat2021mntdp,ostapenko2021localmodulecomposition}.

Rather than adding many new parameters, it's also possible to only add a few parameters that will
adapt the existing network \citep{rebuffi2017visualadapters}: \cite{wen2020batchensemble} and
\cite{sun2019metatransfer} proposed to share most of the weights across tasks, but have
task-specific weights that directly modify the shared weights.

Mixture of experts \citep{masoudnia2014mixture} have also been proposed, where multiple experts
combine their decision. \cite{aljundi2017experts} learn a gating system to use the right
task-specific expert. \cite{yan2021der} and \cite{li2021preserve} proposed to have a \ac{ConvNet}
per task, and to concatenate their output embeddings to be fed to a single classifier. To avoid
parameters explosion, they aggressively prune each \ac{ConvNet}.

\subsection{Classifier correction}
\label{sec:related_classif_correction}

Forgetting happen in both the feature extractor and the classifier. Previously described
rehearsal and regularization methods try to reduce it in both places. On the other hand, multiple
works focused solely on the classifier. They remarked that in \acf{CIL}, the classifier is
miscalibrated \citep{guo2017miscalibration} where the model over-predicts new classes to the
detriment of old classes. \cite{belouadah2019il2m} compensate the bias towards new classes by
rectifying predictions of past classes using their recorded accuracies and confidences.
\cite{wu2019bias_correction} learn a linear model on validation data to recalibrate the logits of
the new classes. \cite{zhao2020weightalignement} normalizes the norm of the classifier weights
associated to new classes so that their average norm becomes the same as that for old classes.
\cite{hou2019ucir}, aims for a similar result, by replacing the dot product in the classifier by a
cosine similarity, resulting in classifier weights of unit norm for all.

\section{Specific variations of Continual Learning}
\label{sec:related_variation}

While \acf{CIL} is the most common benchmark in Continual Learning, there are multiple variations of
benchmark regardless which kind of shift is involved.

\subsection{Multiple labels}
\label{sec:related_multiple_labels}


The main task in Continual Learning is classification of a single class per sample, however it can
also be expanded to multiple classes per samples, \eg object detection
\citep{shmelkov2017incrementalobjectdetection} and semantic segmentation
\citep{michieli2019ilt,cermelli2020modelingthebackground}. The latter has seen recently interest
from the community for its concrete application: hand labeling in segmentation is extremely costly,
and continual segmentation propose to labelize only the new classes in an image, reducing greatly
the labeling cost. In that situation, a segmentation maps (made of one label per pixel) will only be
partially labelized: new classes are labelized, but old classes are assumed to be background.
Moreover, our model may have encountered new classes in the past, when they were themselves
considered as background. It's a case of concept shift, where the conditional distribution $p(y |
      x)$ changes through time. I detail in \autoref{chapter:segmentation} the existing benchmark in
\ac{CSS} and describe how we tackled this problem.

\subsection{Online Learning \& Task drift detection}
\label{sec:related_online_learning}

In Continual Learning, a model learns for multiple epochs for each task. On the other hand, in
Online Learning, also called Stream Learning, there are no notions of tasks nor epochs: a model must
learn on a stream of samples incoming one by one, and which cannot be replayed by epochs \citep{aljundi2019notaskboundaries}. The
methods to reduce forgetting described in \autoref{sec:related_methods} can still be applied in
Online Learning. Modified versions of rehearsal learning, often inspired by reservoir sampling \cite{knuth97tacpvol2}, are
often used \cite{hayes2019exstream,aljundi2019taskfree}.

\begin{figure}[tb]
      \begin{center}
            \includegraphics[width=0.6\linewidth]{images/related/loss_drift.png}
      \end{center}
      \caption{\textbf{Task-free detection of drift in the input distribution} by recording the
            plateau in the loss followed by a peak. y-axis is the loss value, and x-axis the update steps.
            Image from \cite{aljundi2019taskfree}.}
      \label{fig:related_lossdrift}
\end{figure}

Multiple regularization methods (\autoref{sec:related_regul}) needs to do some computation between
tasks. For example, weight-based regularization (\autoref{sec:related_regul_weight}) must compute
the task-specific importance weights. Because in Online Learnig, there are no clear task separation,
a heuristic must determine when doing this computation. \cite{aljundi2019taskfree}, working a stream
of images from soap operas, proposed to analyze the loss surface to find drift in the distribution:
at some point the model is experienced enough, and the loss starts to plateau. When a drift happens,
the loss will usually peak. This is a sign of task-free drift of the distribution as
illustrated in \autoref{fig:related_lossdrift}.

\subsection{Continual-Meta Learning}
\label{sec:related_meta}


Continual Learning aims to not forget. However, we --as humans-- often forget, but we can also
re-learn what was lost quicker than the first time. The goal of \ac{MCL} is therefore to recover as
quickly as possible --sample wise-- the original performance on past tasks
\citep{he2019metacontinual}. As the name implies, meta-learning methods, that aims to \textit{learn
      how to learn}, such as the MAML model \citep{finn2017maml} are used to that end. Then, \ac{MCL} has
been extended to a more general framework where the model also has to adapt quickly to new \acf{OoD}
tasks.

Note that \acf{MCL} is not to be confused with \acf{CML} where in that case meta-learning is only
used during pretraining to provide better model initialization.

\subsection{Zeroshot Continual Learning}
\label{sec:related_zeroshot}


In Computer Vision, \acf{ZSL} \citep{lampert2009zeroshot,xian2019awa2} aims to classify classes that
were never seen before. To do so, models usually exploit an external knowledge source as a word2vec
embedding \citep{mikolov2013word2vec} trained on Wikipedia or an attribute matrix. Several works
have proposed to unify both Continual Learning and \ac{ZSL} where the future classes that haven't
been seen yet must be classified
\cite{lopezpaz2017gem,wei2020lifelongzeroshot,gautam2020continualzeroshot}. Rather than simply using
\ac{ZSL} as end to itself, we proposed, in \citep{douillard2020ghost}, to exploit its properties to
directly avoid forgetting. This is elaborated further in \autoref{chapter:regularization}
(\autoref{sec:ghost}).

\subsection{Natural Language Processing}
\label{sec:related_nlp}


Continual Learning can be applied to all modalities. After \acf{CV}, the most common one is
\ac{NLP}. \ac{NLP} saw its "\textit{ImageNet moment}" with the advent of Transformers
\citep{vaswani2017transformer}, and more recently with multi-tasks learning \citep{raffel2019t5}.
Continual \ac{NLP} \cite{biesialska2020continualnlp} aims naturally to learn multiple tasks, but in
a consecutive fashion with no --or few-- replay of the old tasks data. Applications can be similar
to \ac{CV} with addition of new classes \citep{masson2019episodiclifelongnlp} or new domains (\eg
medical corpus, fiction, tweets, etc.) \citep{gerald2021continualri}.

\subsection{Reinforcement Learning}
\label{sec:related_rl}


\ac{RL} \citep{sutton1998rl} more often than not needs support from Continual
Learning \citep{khetarpal2020continualrl}: for example as an agent evolves in an environment, it
usually needs rehearsal learning (also known as episodic memory) \citep{mnih2013atarirl}. Overall,
the methods originally developped for \ac{CV} (\autoref{sec:related_methods}) were then applied to
both \ac{NLP} and \ac{RL}.
